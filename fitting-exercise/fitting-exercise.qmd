---
title: "Data Fitting Exercise"
author: "Vincent Nguyen"
date: "February 27, 2025"
---

### Loading Data

First, start with loading the data using a relative path.

```{r}
# Load packages
library(ggplot2)
library(dplyr)
library(purrr)

# Load the data
data_location <- here::here("fitting-exercise", "data", "Mavoglurant_A2121_nmpk.csv")
data <- read.csv(data_location)

```

### Preliminary Graph

This section graphs out DV over time grouped by dosage.

```{r}
# Graph DV on y-axis and time on x
ggplot(data, aes(x = TIME, y = DV, group = ID, color = as.factor(DOSE))) +
  geom_line(alpha = 0.6) +
  geom_point(alpha = 0.6) +
  facet_wrap(~ DOSE) +  
  labs(title = "DV Over Time by Dose",
       x = "Time",
       y = "DV",
       color = "Dose") +
  theme_minimal()


```

### Data Cleaning

Cleaning data as directed by the assignment guidelines. Specifically, the final data set only contains DV, dosage, age, sex, race, weight, and height. Additionally, filtered out occurrences of individuals receiving more than one dose.

```{r}
# Keep rows where OCC = 1 only and time does not equal 0
data_filtered <- data %>%
  filter(OCC == 1, TIME != 0)

sum_dv <- data_filtered %>%
  group_by(ID) %>%
  summarise(Y = sum(DV))

data_zero <- data %>%
  filter(OCC ==1, TIME == 0)

data_final <- data_zero %>%
  left_join(sum_dv, by = "ID")

# Convert RACE and SEX into factors
data_final$RACE <- as.factor(data_final$RACE)
data_final$SEX <- as.factor(data_final$SEX)

# Select specific variables of interest
data_final <- data_final %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT)
```

### Exploratory Data Analysis

\
Created a summary table using the skimr package. I personally find this summary table to be more informative than the base summary().

```{r}
library(skimr)
skim(data_final)
```

As part of the exploratory data analysis, I created a boxplot detailing DV per race. For the most part, the medians of each race seem similar, however, the range of DV values for Race 1 is extremely bigger than 7 or 88. Race 7 also has the smallest spread overall.

```{r}
histogram_race <- ggplot(data_final, aes(x = RACE, y = Y)) +
    geom_boxplot(fill = "skyblue", color = "black") +
    labs(title = "Boxplot of Y by Race", x = "Race", y = "Y") +
  theme_minimal()

print(histogram_race)
```

I calculated correlations using Pearson's for the continuous variables of Y, dose, weight, height, and age. I also created a correlation heat map (with the help of ChatGPT). Overall, not much seems to be correlated besides Y and dosage. As expected, height and weight have a slight correlation.

```{r}

library(reshape2)
library(RColorBrewer)

# Calculation of correlations
cor_matrix = cor(data_final[,c("Y", "DOSE", "WT", "HT", "AGE")], method = "pearson")
print(cor_matrix)

# Melt the correlation matrix for ggplot
cor_matrix_melted <- melt(cor_matrix)

# Plot the heatmap
ggplot(cor_matrix_melted, aes(Var1, Var2, fill = value)) + 
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Heatmap", x = "Variables", y = "Variables")
```

### Model Creation and Fitting

This section details the creation of the models. First, we started with linear models.

The model with only dosage as a predictor of Y resulted in a R-squared of 0.515 which means 51.5% of the variance in Y is explained by the model. The RMSE of this model is 666 meaning that, on average, the model's predictions are off by 666 units; this figure may require more context before deciding if this is good or not.

The model with all predictors resulted in a R-squared of 0/754 which means 75.4% of the variance in Y is explained by the model. The RMSE of this model is 474 meaning that, on average, the model's predictions are off by 666 units; this figure may require more context before deciding if this is good or not.

Overall, the second model, with dosage, age, sex, race, weight, and height, as predictors, performed much better than the more basic model. A R-squared of 0.754 does indicate room for improvement.

```{r}
library(tidymodels)

# set method for modeling
lm_mod <- linear_reg() %>%
  set_engine("lm")

# create model with only dose as predictor of y
lm_y_dose <- lm_mod %>%
  fit(Y ~ DOSE, data = data_final)
lm_y_dose

# create model with all predictors for y
lm_y_all <- lm_mod %>%
  fit(Y ~ DOSE * AGE * SEX * RACE * WT * HT, data = data_final)
lm_y_all

# create predictions based on the model, use for computation of metrics
dose_predictions_lm <- predict(lm_y_dose, new_data = data_final)
all_predictions_lm <- predict(lm_y_all, new_data = data_final)

# Calculate RMSE and R-squared for dose model
metrics_dose_lm <- tibble(truth = data_final$Y, predicted = dose_predictions_lm$.pred) %>%
  metrics(truth = truth, estimate = predicted)
metrics_dose_lm

# Calculate RMSE and R-squared for model with all predictors
metrics_all_lm <- tibble(truth = data_final$Y, predicted = all_predictions_lm$.pred) %>%
  metrics(truth = truth, estimate = predicted)
metrics_all_lm
```

Second, we created logistic regression models to predict sex instead.

The first model only use dosage as a predictor of sex. It has an accuracy of 86.66% and kappa of 0. ChatGPT tells me the model does not perform better than random guessing chance when accounting for random agreement between predictors and true labels. The area under the curve is 0.591 indicating poor to fair predictive performance. The model requires improvements.

The second model uses dosage, age, race, weight, and height to predict sex. With an accuracy of 98.33% and kappa of 0.93, the model performs well in raw accuracy and its ability to make predictions that surpasses random chance. Additionally, the area under the curve is 0.9982 which means that the model is very good at distinguishing the two classes of sex. This model is much better than the original one.

```{r}
# set method for modeling
log_mod <- logistic_reg() %>%
  set_engine("glm")

# create model with only dose as predictor of SEX
log_dose <- log_mod %>%
  fit(SEX ~ DOSE, data = data_final)
log_dose

# create model with all predictors for SEX
log_all <- log_mod %>%
  fit(SEX ~ DOSE * AGE * RACE * WT * HT, data = data_final)
log_all

# create predictions based on the model, use for computation of metrics
dose_predictions_log <- predict(log_dose, new_data = data_final, type = "prob")
all_predictions_log <- predict(log_all, new_data = data_final, type = "prob")

dose_predictions_class <- predict(log_dose, new_data = data_final)
all_predictions_class <- predict(log_all, new_data = data_final)

# Compute accuracy based off log_dose
dose_accuracy <- metrics(data = tibble(truth = data_final$SEX, predicted = dose_predictions_class$.pred_class), 
                    truth = truth, estimate = predicted)
dose_accuracy

# Compute accuracy based off log_all
all_accuracy <- metrics(data = tibble(truth = data_final$SEX, predicted = all_predictions_class$.pred_class), 
                    truth = truth, estimate = predicted)
all_accuracy

# Compute ROC/AUC for log_dose
roc_auc_dose <- roc_auc(data = tibble(truth = data_final$SEX, 
                                 .pred_1 = dose_predictions_log$.pred_1), 
                   truth = truth, .pred_1)
print(roc_auc_dose)

# Compute ROC/AUC for log_all
roc_auc_all <- roc_auc(data = tibble(truth = data_final$SEX, 
                                 .pred_1 = all_predictions_log$.pred_1), 
                   truth = truth, .pred_1)
print(roc_auc_all)


```

Week 10 Exercise starts!

Seeding and Splitting

```{r}
# Set seed
rngseed = 1234

# Filter data to include columns of interest
data_ten <- select(data_final,Y, DOSE, AGE, SEX, WT, HT)

# Set seed for splitting
set.seed(rngseed)

# Split data into training and testing
# Put 3/4 of data into training set
data_split <- initial_split(data_ten, prop = 3/4)

train_data <- training(data_split)
test_data <- training(data_split)
```

Model Fitting

This section entails the creation of models but with the train_data instead of the original data set.

```{r}
# set method for modeling
lm_mod <- linear_reg() %>%
  set_engine("lm")

# create model with only dose as predictor of y
lm_y_dose <- lm_mod %>%
  fit(Y ~ DOSE, data = train_data)
lm_y_dose

# create model with all predictors for y
lm_y_all <- lm_mod %>%
  fit(Y ~ DOSE * AGE * SEX * WT * HT, data = train_data)
lm_y_all

# Create null model
null_mod <- null_model(mode = "regression") %>%
  set_engine("parsnip") %>%
  fit(Y ~ 1, data = train_data)
```

Model Performance Assessment 1

This section entails the computation of RMSE and other stats to analyze model performance.

```{r}
# create predictions based on the model, use for computation of metrics
dose_predictions_lm <- predict(lm_y_dose, new_data = train_data)
all_predictions_lm <- predict(lm_y_all, new_data = train_data)
null_predictions <- predict(null_mod, new_data = train_data)

# Calculate RMSE and R-squared for dose model
metrics_dose_lm <- tibble(truth = train_data$Y, predicted = dose_predictions_lm$.pred) %>%
  metrics(truth = truth, estimate = predicted)


# Calculate RMSE and R-squared for model with all predictors
metrics_all_lm <- tibble(truth = train_data$Y, predicted = all_predictions_lm$.pred) %>%
  metrics(truth = truth, estimate = predicted)


# Calculate RMSE and R-squared for null model
metrics_null_lm <- tibble(truth = train_data$Y, predicted = null_predictions$.pred) %>%
  metrics(truth = truth, estimate = predicted)

# Compare the metrics
metrics_dose_lm
metrics_all_lm
metrics_null_lm
```

Model Performance Assessment 2

This section utilizes 10 fold cross-validation to assess the models. The RMSE of the model with all predictors greatly increased, indicating great overfitting.

```{r}
# Set seed
set.seed(1234)

# Set seed for repeat
set.seed(4321)

# Create folds
folds <- vfold_cv(train_data, v = 10)
folds

# Dose Model Workflow
dose_cv <- workflow() %>%
  add_model(lm_mod) %>%
  add_formula(Y ~ DOSE)

# Dose Model Resample
dose_cv_rs <- dose_cv %>%
  fit_resamples(folds)

# Metrics for Dose model
dose_metrics <- collect_metrics(dose_cv_rs)

# All variables Model Workflow
all_cv <- workflow() %>%
  add_model(lm_mod) %>%
  add_formula(Y ~ DOSE * AGE * SEX * WT * HT)

# All Model Resample
all_cv_rs <- all_cv %>%
  fit_resamples(folds)

# Metrics for All model
all_metrics <- collect_metrics(all_cv_rs)

dose_metrics
all_metrics

```

## Part 2

### This section was contributed by Connor Norris

```{r}
#Put predicted and observed values of each model in a new data frame
#Data frame for model fitting Y from dose
model1_res <- data.frame(
  truth = train_data$Y,
  pred = dose_predictions_lm$.pred,
  model = rep(1, nrow(train_data))
)

#Data frame for model fitting Y from all predictors
model2_res <- data.frame(
  truth = train_data$Y,
  pred = all_predictions_lm$.pred,
  model = rep(2, nrow(train_data))
)

#Data frame for null model
null_mod_res <- data.frame(
  truth = train_data$Y,
  pred = null_predictions$.pred,
  model = rep("null", nrow(train_data))
)

#Add all model results together
model_res <- rbind(model1_res, model2_res, null_mod_res)

#Plot predicted vs. observed values for all models
ggplot(data = model_res, aes(x = truth, y = pred, colour = model)) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) +
  labs(
    x = "Observed Value",
    y = "Predicted Value",
    title = "Observed vs. Predicted Values of Y from 3 Models",
    colour = "Model"
  ) +
  scale_x_continuous(limits = c(0, 5000)) +
  scale_y_continuous(limits = c(0, 5000))
```

```{r}
#Calculate residuals for model 2
model2_res <- mutate(model2_res, residuals = pred - truth)

#Plot the residuals of model 2
ggplot(model2_res, aes(x = pred, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0) + 
  labs(
    x = "Predicted Values",
    y = "Residuals",
    title = "Predicted Values vs. Residuals"
  )
```

```{r}
#Reset seed
set.seed(rngseed)

#Generate bootstrap samples
bootstraps <- bootstraps(data = train_data, times = 1000)

#Fit the model and make predictions for each bootstrap sample
predictions_list <- map(bootstraps$splits, function(split) {
  #Extract the bootstrap sample
  boot_data <- analysis(split)
  
  #Fit the model (replace this with your model)
  model <- lm_mod %>%
    fit(Y ~ ., data = boot_data)
  
  # Make predictions on the original training data
  preds <- predict(model, new_data = train_data)
  
  return(preds)
})

#Convert the list to a data frame for easier handling
predictions_df <- as.data.frame(do.call(cbind, predictions_list))

#View first few rows
head(predictions_df)

#Transpose to have samples as rows and data points as columns
predictions_df <- t(predictions_df)

#Calculate median and 95% confidence intervals
preds <- predictions_df |> apply(2, quantile,  c(0.025, 0.5, 0.975)) |> t()
```

```{r, warning=FALSE}
#Make a data frame for plotting
plot_data <- data.frame(
  truth = train_data$Y,
  point_estimate = model2_res$pred,
  median = preds[,2],
  lower_ci = preds[,1],
  upper_ci = preds[,3]
)

#Plot observed values vs. estimates
ggplot(data = plot_data, aes(x = truth)) +
  geom_point(aes(y = point_estimate, color = "Point Estimate"), size = 2) + #Original predicted value
  geom_point(aes(y = median, color = "Median Prediction"), size = 2) + #Median predicted value from bootstrapping
  geom_point(aes(y = lower_ci, color = "Lower 95% CI"), size = 1, alpha = 0.7) + #Lower bound of 95% CI
  geom_point(aes(y = upper_ci, color = "Upper 95% CI"), size = 1, alpha = 0.7) + #Upper bound of 95% CI +
  geom_abline(intercept = 0, slope = 1) +
  scale_x_continuous(limits = c(0, 5000)) +
  scale_y_continuous(limits = c(0, 5000)) +
  labs(
    x = "Observed Values",
    y = "Predictions",
    title = "Observed Values vs. Predictions"
  ) +
  scale_color_manual(
    name = "Legend", 
    values = c(
      "Point Estimate" = "black",
      "Median Prediction" = "blue",
      "Lower 95% CI" = "red",
      "Upper 95% CI" = "purple"
    )
  ) + 
  theme_minimal()
```

The bootstrapping predictions perform about the same as the original model with all predictors, as the median values largely overlap with the point estimates, and their distributions about the 45 degree line are similar.

Part 3

This section was done before Part 2's completion.

```{r}
# Generating Predictions from Test data
test_lm_two <- predict(lm_y_all, new_data = test_data)

# calculate metrics from test data
rmse_test_lm_two <- tibble(truth = test_data$Y, predicted = test_lm_two$.pred)%>%
  metrics(truth = truth, estimate = predicted)

# output
rmse_test_lm_two

```

```{r}
# Plot
# create df for plotting
train_plot_data <- tibble(Observed = train_data$Y, Predicted = all_predictions_lm$.pred, Dataset = "Training")
test_plot_data <- tibble(Observed = test_data$Y, Predicted = test_lm_two$.pred, Dataset = "Test")

# combine dfs
plot_data <- bind_rows(train_plot_data, test_plot_data)

# plot predicted vs observed with different colors for training and test
ggplot(plot_data, aes(x = Observed, y = Predicted, color = Dataset)) +
  geom_point(alpha = 0.5, size = 2) +  # Reduce opacity and increase point size for overlapping points
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("Training" = "red", "Test" = "blue")) +
  labs(title = "Predicted vs Observed Values",
       x = "Observed Y",
       y = "Predicted Y",
       color = "Dataset") +
  theme_minimal()


```

This week's exercise started with seed setting to ensure reproducibillity and creating a new data frame with variables of interest using week eight's data. After this, the data was split into a 75% training and 25% testing set. The models from week eight (Model 1 = just dose as predictor. Model 2 = dose, age, sex, weight, and height as predictors) were fitted using the training data. These models were the nassessed using RMSE. Additionally, a null model was created. Model 2 performed the best, having the lowest RMSE.

After the intial fitting, a 10-fold cross-validation was done. This process fitted the two models ten times each, constituing about 90% of the data. The other 10% was used to evaluate the fit by computing the RMSE of the model.

Part 2 was not completed at the time of writing this.

Regarding overall model assessment, both models performed better (in terms of RMSE) than the null model. Model 1 having better results than the null makes sense. Presumably, knowing the dose helps towards predicting Y rather than no predictors at all. This could be difficult to use for real applications as the model is very simple. Model 2 also improves upon model 1. Logically, this makes sense considering the factors, like height and weight, that go into drug concentration. I would consider the model usable as the predictors are generally easy to measure and have logical sense in predicting drug concentration.The model is also interpretable as there is not too many predictors involved.
