[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "Alexis Gonzalez contributed to this project.\n\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nThe analysis intends to seek if there associations between shoe size and preference to the physical characteristics of an individual like their height or weight."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nThe data file, “exampledata2”, is located in the same folder as “exampledata”. Along with the same data found in “exampledata”, “exampledata2” contains two new columns, one numeric and one character. The first column is shoe size (specifically Men’s US sizing) and the second column is favorite shoe color. For shoe color, some colors show up more than others but I believe people tend to enjoy neutral colors more."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nThe project analyzes characteristics like height, weight, shoe size, and favorite shoe size to find any associations. There is simple descriptive analysis but also linear model creation.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nFavorite Shoe Color\n0\n1\n4\n5\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.666667\n15.976545\n133\n156.0\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.111111\n21.245261\n45\n55.0\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nShoe Size (US/M)\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7.611111\n2.368778\n4\n6.5\n7\n10\n11\n▅▂▇▁▇"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\n(Boxplot?) shows a box-plot figure produced by one of the R scripts. While limited by a small sample size, the figure shows that individuals with black or white as their favorite shoe color have a large variation in height.\n\n\n\n\n\nHeight stratified by favorite shoe color\n\n\n\n\n(Scatter?) shows a box-plot figure produced by one of the R scripts. Judging by the few data points available, there tends to be a positive association between shoe size and weight. Like the rest of the results, there is no significance detected here.\n\n\n\n\n\nWeight stratified by shoe size."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\nTable 3 shows a summary of a linear model fit. While this analysis is silly and insignificant, I will highlight some findings anyway. First, if an individual had a shoe size of 0, their height would be 134.8 cm. Next, for every increase in shoe size, there is a 3.8 cm increase in height. Additionally, individuals with their favorite shoe color as black tended to be 13.7 cm taller than those with favorite shoe color of white.\n\n\n\n\nTable 3: Linear model fit table for Height as outcome and Shoe Size and Favorite Shoe color as predictors.\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n143.839286\n23.600665\n6.0947132\n0.0088725\n\n\nShoe Size (US/M)\n3.803571\n2.796511\n1.3601133\n0.2669785\n\n\nFavorite Shoe ColorBlue\n-3.875000\n20.781334\n-0.1864654\n0.8639765\n\n\nFavorite Shoe ColorBrown\n-14.562500\n19.998256\n-0.7281885\n0.5191891\n\n\nFavorite Shoe ColorGreen\n-4.464286\n19.818211\n-0.2252618\n0.8362481\n\n\nFavorite Shoe ColorWhite\n-13.732143\n13.982553\n-0.9820912\n0.3984739"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.2.3\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/86182/Desktop/EPID 8060E/MADA/vincentnguyen-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height                &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\",…\n$ Weight                &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 4…\n$ Gender                &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M…\n$ `Shoe Size (US/M)`    &lt;dbl&gt; 11.0, 10.0, 5.5, 10.0, 13.0, 5.0, 7.0, 8.0, 8.5,…\n$ `Favorite Shoe Color` &lt;chr&gt; \"Black\", \"White\", \"Green\", \"Blue\", \"Brown\", \"Bla…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          Shoe Size (US/M)\n Length:14          Min.   :  45.0   Length:14          Min.   : 4.000  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.: 6.500  \n Mode  :character   Median :  70.0   Mode  :character   Median : 7.250  \n                    Mean   : 602.7                      Mean   : 7.821  \n                    3rd Qu.:  90.0                      3rd Qu.: 9.625  \n                    Max.   :7000.0                      Max.   :13.000  \n                    NA's   :1                                           \n Favorite Shoe Color\n Length:14          \n Class :character   \n Mode  :character   \n                    \n                    \n                    \n                    \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Shoe Size (US/M)` `Favorite Shoe Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;                \n1 180        80 M                    11   Black                \n2 175        70 O                    10   White                \n3 sixty      60 F                     5.5 Green                \n4 178        76 F                    10   Blue                 \n5 192        90 NA                   13   Brown                \n6 6          55 F                     5   Black                \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nFavorite Shoe Color\n0\n1\n4\n5\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.0\n70.00\n90.00\n7000\n▇▁▁▁▁\n\n\nShoe Size (US/M)\n0\n1.00\n7.82\n2.48\n4\n6.5\n7.25\n9.62\n13\n▅▇▃▅▂\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nFavorite Shoe Color\n0\n1\n4\n5\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165.0\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73.0\n90\n7000\n▇▁▁▁▁\n\n\nShoe Size (US/M)\n0\n1.00\n8.00\n2.48\n4\n6.50\n7.5\n10\n13\n▃▇▃▅▂\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nFavorite Shoe Color\n0\n1\n4\n5\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166.0\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73.0\n90\n7000\n▇▁▁▁▁\n\n\nShoe Size (US/M)\n0\n1.00\n8.00\n2.48\n4\n6.50\n7.5\n10\n13\n▃▇▃▅▂\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nFavorite Shoe Color\n0\n1\n4\n5\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.50\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.50\n70\n85\n110\n▇▂▃▃▂\n\n\nShoe Size (US/M)\n0\n1\n8.18\n2.67\n4\n6.75\n8\n10\n13\n▅▇▅▇▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nFavorite Shoe Color\n0\n1\n4\n5\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.50\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.50\n70\n85\n110\n▇▂▃▃▂\n\n\nShoe Size (US/M)\n0\n1\n8.18\n2.67\n4\n6.75\n8\n10\n13\n▅▇▅▇▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nFavorite Shoe Color\n0\n1\n4\n5\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156.0\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55.0\n70\n80\n110\n▇▂▃▂▂\n\n\nShoe Size (US/M)\n0\n1\n7.61\n2.37\n4\n6.5\n7\n10\n11\n▅▂▇▁▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/86182/Desktop/EPID 8060E/MADA/vincentnguyen-MADA-portfolio\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.2.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable       n_missing complete_rate min max empty n_unique whitespace\n1 Favorite Shoe Color         0             1   4   5     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable    n_missing complete_rate   mean    sd  p0   p25 p50 p75 p100\n1 Height                   0             1 166.   16.0  133 156   166 178  183\n2 Weight                   0             1  70.1  21.2   45  55    70  80  110\n3 Shoe Size (US/M)         0             1   7.61  2.37   4   6.5   7  10   11\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▅▂▇▁▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\np5 &lt;- mydata %&gt;% ggplot(aes(x=`Favorite Shoe Color`, y= Height, color=`Favorite Shoe Color` )) + geom_boxplot()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"Heightcolorstratisfied.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\n\np6 &lt;- mydata %&gt;% ggplot(aes(x= Weight, y=`Shoe Size (US/M)`, color= Weight )) + geom_point()\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"Shoesizesweightstratisfied.png\")\nggsave(filename = figure_file, plot=p6)\n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Placeholder file for the future data/results presentation exercise."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Placeholder file for the future R coding exercise.\nThis coding exercise begins with loading and checking the data using help(), str(), summary(), and class().\n\n# Loading and Checking Data\n\n# loading dslabs package\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.3\n\n# look at help file for gapminder data\nhelp(gapminder)\n\nstarting httpd help server ... done\n\n# get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n# get a summary of the data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n# determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\nThis part of the exercise is titled, “Processing Data”, where objects are creating using the gapminder data set.\n\n# Processing Data\n\n# load tidyverse package\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# create new object/variable called africadata\nafricadata &lt;- gapminder %&gt;%\n  filter(continent == \"Africa\")\n\n# create a new object on only infant_mortality\ninfant_health &lt;- africadata %&gt;%\n  select(infant_mortality, life_expectancy)\n\n# Create a new object containing population and life_expectancy\noverall_health &lt;- africadata %&gt;%\n  select(population, life_expectancy)\n\n# inspect new objects\nstr(infant_health)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(infant_health)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\nstr(overall_health)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(overall_health)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n\nThis part of the exercise is titled, “Plotting”, and focuses on creating plots on the objects previously created. For the graph, “Life expectancy as a function of Population Size in Africa”, there is a streaking pattern which is caused by countries having several entries in different years. Countries tended to increase in life expectancy and size as years pass.\n\nlibrary(ggplot2)\n\n# create a scatter plot of life expectancy as a function of infant mortality\ninfant_graph &lt;- ggplot(africadata, aes(x = infant_mortality, y =life_expectancy)) + geom_point(alpha = 0.6, color = \"blue\") + labs(\n  title = \"Life expectancy as a function of Infant Mortality in Africa\",\n  x = \"Infant Mortality\",\n  y = \"Life Expectancy\"\n)\n\n# display graph\nprint(infant_graph)\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# create a scatter plot of life expectancy as a function of population size\npopulation_graph &lt;- ggplot(africadata, aes(x = population, y = life_expectancy)) + geom_point(alpha = 0.6, color = \"blue\") + scale_x_log10() + labs(\n  title = \"Life expectancy as a function of Population Size in Africa\",\n  x = \"Population (Log Scale)\",\n  y = \"Life Expectancy\"\n)\n\n# display graph\nprint(population_graph)\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis part of the exercise is titled, “More data processing”. In this part, the data is inspected to figure out which years have the most missing counts. The assignment chooses the year 2000 to filter the data. I create an object called africadata_twothousand which contains the filtered data.\n\nna_count_per_year &lt;- africadata %&gt;%\n  filter(is.na(infant_mortality)) %&gt;%\n  group_by(year) %&gt;%\n  summarize(na_count_per_year = n())\n\n# print result\nprint(na_count_per_year)\n\n# A tibble: 23 × 2\n    year na_count_per_year\n   &lt;int&gt;             &lt;int&gt;\n 1  1960                10\n 2  1961                17\n 3  1962                16\n 4  1963                16\n 5  1964                15\n 6  1965                14\n 7  1966                13\n 8  1967                11\n 9  1968                11\n10  1969                 7\n# ℹ 13 more rows\n\n# create new object with only the year 2000\nafricadata_twothousand &lt;- africadata %&gt;%\n  filter(year == \"2000\")\n\n# check/inspect new object with data from the year 2000\nstr(africadata_twothousand)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata_twothousand)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n\nThis part of the exercise is called, “More plotting”. This is essentially identidical as a previous section but utilizes filtered data for only the year 2000.\n\n# create a scatter plot of life expectancy as a function of infant mortality\ninfant_graph_twothousand &lt;- ggplot(africadata_twothousand, aes(x = infant_mortality, y =life_expectancy)) + geom_point(alpha = 0.6, color = \"blue\") + labs(\n  title = \"Life expectancy as a function of Infant Mortality in Africa in the year 2000\",\n  x = \"Infant Mortality\",\n  y = \"Life Expectancy\"\n)\n\n# display graph\nprint(infant_graph_twothousand)\n\n\n\n\n\n\n\n# create a scatter plot of life expectancy as a function of population size\npopulation_graph_twothousand &lt;- ggplot(africadata_twothousand, aes(x = population, y = life_expectancy)) + geom_point(alpha = 0.6, color = \"blue\") + scale_x_log10() + labs(\n  title = \"Life expectancy as a function of Population Size in Africa in the year 2000\",\n  x = \"Population (Log Scale)\",\n  y = \"Life Expectancy\"\n)\n\n# display graph\nprint(population_graph_twothousand)\n\n\n\n\n\n\n\n\nThis part of the exercise is called, “Simple model fits”. This code creates a linear model using lm(). Model 1, “Fit1”, is a model with life expectancy as the outcome and infant mortality as the predictor. This model has a p-value of 2.826e-08. This indicates significance. Model 2, “Fit2”, is a model with population size as a predictor and life expectancy as the outcome. This model has a p-value of 0.6159. This indicates insignificance.\n\n# Fit Model 1 - Life Expectancy and Infant Mortality\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africadata_twothousand)\n\n# summary of fit model 1\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata_twothousand)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n# Fit Model 2 - Life Expectancy and Population Size\nfit2 &lt;- lm(life_expectancy ~ population, data = africadata_twothousand)\n\n# summary of fit model 2\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africadata_twothousand)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\n\nThe following sections contributed by Guozheng Yang\n\n\nLoad packages and dataset\nI’m going to explore the murders dataset from the package dslabs. This dataset contains gun murder data for 2010 reported by FBI, and is sourced from Wikipedia. The dataset is organized by states, with the population of each state included.\n\n# Load required package\nlibrary(dslabs) # This package has the murders dataset\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(maps) # This package is used to make a map\n\nWarning: package 'maps' was built under R version 4.3.3\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n# Look at help file for murders data\nhelp(murders)\n\nFirst of all, let’s take a look at the data structure.\n\n# Determine the type of murders\nclass(murders)\n\n[1] \"data.frame\"\n\n# Get an overview of data structure\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ...\n\n# Get a summary of data\nsummary(murders)\n\n    state               abb                      region     population      \n Length:51          Length:51          Northeast    : 9   Min.   :  563626  \n Class :character   Class :character   South        :17   1st Qu.: 1696962  \n Mode  :character   Mode  :character   North Central:12   Median : 4339367  \n                                       West         :13   Mean   : 6075769  \n                                                          3rd Qu.: 6636084  \n                                                          Max.   :37253956  \n     total       \n Min.   :   2.0  \n 1st Qu.:  24.5  \n Median :  97.0  \n Mean   : 184.4  \n 3rd Qu.: 268.0  \n Max.   :1257.0  \n\n# Check if there are missing values\nanyNA(murders)\n\n[1] FALSE\n\n\nAs shown, there are 51 observations and 5 variables in this dataset. 51 states and their abbreviations are listed. The states are also classified by their geographic locations. Two numeric variables denote the population and number of gun murders of each state in 2010. Luckily, we don’t have any NA in this dataset.\n\n\nMap plotting\nSince we have data from different states, I want to make a map to compare the different gun murder rates by states. Here I calculate the gun murder rates as the number of gun murders divided by the population of each state. Then I use a heatmap to color the statesby their gun murder rates. Of note, the US map data is in the package maps, with the longitude and latitude of each state included. This tool makes it convenient for us to draw the heatmap.\nLet’s take a look at the data from package maps first.\n\n# Extract the US map data\nus_map &lt;- map_data(\"state\")\n\n# Determine the type of murders\nclass(us_map)\n\n[1] \"data.frame\"\n\n# Get an overview of data structure\nstr(us_map)\n\n'data.frame':   15537 obs. of  6 variables:\n $ long     : num  -87.5 -87.5 -87.5 -87.5 -87.6 ...\n $ lat      : num  30.4 30.4 30.4 30.3 30.3 ...\n $ group    : num  1 1 1 1 1 1 1 1 1 1 ...\n $ order    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ region   : chr  \"alabama\" \"alabama\" \"alabama\" \"alabama\" ...\n $ subregion: chr  NA NA NA NA ...\n\n# Get a summary of data\nsummary(us_map)\n\n      long              lat            group           order      \n Min.   :-124.68   Min.   :25.13   Min.   : 1.00   Min.   :    1  \n 1st Qu.: -96.22   1st Qu.:33.91   1st Qu.:15.00   1st Qu.: 3899  \n Median : -87.61   Median :38.18   Median :26.00   Median : 7794  \n Mean   : -89.67   Mean   :38.18   Mean   :30.15   Mean   : 7798  \n 3rd Qu.: -79.13   3rd Qu.:42.80   3rd Qu.:47.00   3rd Qu.:11699  \n Max.   : -67.01   Max.   :49.38   Max.   :63.00   Max.   :15599  \n    region           subregion        \n Length:15537       Length:15537      \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nAs shown, all state names are in stored in the region variable in lower case. Variables long and lat have the longitude and latitude of each state’s border. As this is a well prepared data, I will calculate gun murder rates in the murders dataset and then merge it into us_map.\n\n# Prepare the murders data\nmurders &lt;- murders %&gt;%\n  mutate(state_low=tolower(state), # Convert state names to lower case\n         rate=total/population) # Calculate gun murder rate\n\nmap_data &lt;- us_map %&gt;%\n  left_join(murders, by=c(\"region\"=\"state_low\"))\n\nAs the data for plotting is well prepared, I will use geom_polygon() function to draw the map and then color each state by gun murder rates. The geom_polygon() function is a useful tool for connecting a group of points by a certain order. In our case, it’s used to draw the border line of each state.\n\n# Draw a heatmap of gun murder rates\nus_gmr &lt;- ggplot(map_data, aes(x=long, y=lat, group=group, fill=rate))+\n  geom_polygon(color=\"steelblue4\", linewidth=1)+ # Draw border line of states\n  scale_fill_gradient(name=\"Gun murder rate\", \n                      low=\"white\", high=\"firebrick4\")+ # Define color gradient\n  theme_bw()+\n  labs(title=\"Gun murder rate by state\", x=\"Longitude\", y=\"Latitude\")+\n  theme(axis.title.x=element_text(size=10, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=10, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=8),\n        axis.text.y=element_text(color=\"black\", size=8),\n        plot.title=element_text(size=15, color=\"black\", face=\"bold\"),\n        legend.position=\"right\",\n        legend.title=element_text(size=10, face=\"bold\"))\n\nus_gmr\n\n\n\n\n\n\n\n\nAs the map shows, the gun murder rate is higher overall in the southern US. For the northern US, the northwest has a lower gun murder rate than the northeast. This map demonstrate the geographic distribution of gun murder rates.\n\n\nModel fitting\nAs the response of interest is gun murder rate, it’s intuitive to fit a Poisson regression with a log link function. In the Poisson regression model, the number of gun murders is the response, and the population of each state is the offset. The region of states is a predictor to show if gun murder rate is different across different regions.\nBefore that, let’s make a grouped boxplot to see if the difference is evident. Here I use region as the X-axis and gun murder rate as the Y-axis.\n\nboxplot &lt;- ggplot(murders, aes(x=region, y=rate))+\n  geom_boxplot(color=\"firebrick3\", width=.5, linewidth=1)+\n  theme_bw()+\n  labs(title=\"Gun murder rate by region\", x=\"Region\", y=\"Gun murder rate\")+\n  theme(axis.title.x=element_text(size=10, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=10, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=8),\n        axis.text.y=element_text(color=\"black\", size=8),\n        plot.title=element_text(size=15, color=\"black\", face=\"bold\"),\n        legend.position=\"right\",\n        legend.title=element_text(size=10, face=\"bold\"))\nboxplot\n\n\n\n\n\n\n\n\nAs shown, the gun murder rate in southern US is evidently higher compared to other regions. The northeast, north central region, and west of the US have similar gun murder rate, though the median in the west is a little lower.\nNow, let’s fit a Poisson regression model to see if the regional difference is statistically significant. As indicated by the map above, the southern US has a generally lower gun murder rate. So I want to use South as the reference group and adjust the factor levels of region.\n\n# Adjust factor level of region\nmurders$region_fct &lt;- factor(murders$region, levels=c(\"South\", \"North Central\", \"West\", \"Northeast\"))\n\n# Poisson regression: gun murder rate ~ region\npoi_fit &lt;- glm(total ~ region_fct, offset=log(population), family=\"poisson\", data=murders)\nsummary(poi_fit)\n\n\nCall:\nglm(formula = total ~ region_fct, family = \"poisson\", data = murders, \n    offset = log(population))\n\nCoefficients:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -10.22464    0.01544 -662.24   &lt;2e-16 ***\nregion_fctNorth Central  -0.28349    0.02803  -10.12   &lt;2e-16 ***\nregion_fctWest           -0.31140    0.02760  -11.28   &lt;2e-16 ***\nregion_fctNortheast      -0.31162    0.03032  -10.28   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1570.6  on 50  degrees of freedom\nResidual deviance: 1361.1  on 47  degrees of freedom\nAIC: 1684.8\n\nNumber of Fisher Scoring iterations: 4\n\n# Exponential the coefficient for interpretation\nexp(poi_fit$coefficients)\n\n            (Intercept) region_fctNorth Central          region_fctWest \n           3.626558e-05            7.531479e-01            7.324233e-01 \n    region_fctNortheast \n           7.322624e-01 \n\n\nAccording to the output, the mean gun murder rate in southern US is 3.63e-5. The north central US has a 25% decrease in gun murder rate compared to southern US. The western and northeastern US both have about 27% decrease in gun murder rate than southern US. Notably, the coefficient for north central US, western US, and northeastern US are all negative with p-values lower than 0.05. We have enough evidence to reject the null hypothesis and conclude that the gun murder rates in these three regions are significantly lower compared to southern US."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Hello, I am Vincent Nguyen\n\n\nI am 4th year undergraduate student in the Double Dawgs program. I intend to pursue an MPH, specifically in Epidemiology. In the past, I have been involved in research labs that utilize R but definitely not to the extent MADA does. I have dabbled in statistical analysis, machine learning modeling, and more. This course I am to solidify my skills in R (I have used free courses, however, have felt the need for more formal training). I also want to explore more advanced and modern techniques in statistical analysis like machine learning.\n\nFun fact about me is that I love to play games and watch movies. Currently, I am a top ranked player in Fortnite (Top 1000!) and in Marvel Rivals (Top 3%). My favorite movies are Tenet and Star Wars The Force Awakens.\n\n\nCareer\nCurrently, I am involved in the Sundaram Lab at UGA and an intern for the CDC’s Division of Workforce Development. I aim to become an Epidemiologist or Biostatistician with the skills learned from this class! Some of my research interests include oncology, infectious disease ecology, and parasitology.\n\n\n\n\nHighlighted Data Analysis Technique\nThe application of simulated environments for analyzing changes in health policy is something I am greatly interested in. For example, this article details the use of EU-TOPIA, an evaluation tool designed to allow researchers use country-specific data to quantify the potential benefits and harms of diferent cancer guidelines in their respective country. Going forward, I believe the application of simulated environments can help bolster research at a low cost!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vincent Nguyen’s MADA Portfolio",
    "section": "",
    "text": "Created on 01/06/2025\n\nHello,\nWelcome to my website and data analysis portfolio for MADA.\n\nPlease use the Menu Bar above to look around.\nHave fun!"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Vincent Nguyen's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-acquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-acquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data acquisition",
    "text": "3.1 Data acquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "data-exercise/data-exercise.qmd.html",
    "href": "data-exercise/data-exercise.qmd.html",
    "title": "Data Exercise",
    "section": "",
    "text": "First, we will start with loading some necessary packages for data creation, visualization, and more.\n\n# loading dslabs package\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(here)\n\nhere() starts at C:/Users/vince/OneDrive/Desktop/port-MADA/vincentnguyen-MADA-portfolio\n\n\n\n\n\nSince the process is randomized, setting a seed can help improve reproduciblity when creating synthetic data.\n\n# set a seed for reproducibility\nset.seed(123)\n\n# define number of observations\nn_observations &lt;- 100\n\n\n\n\nNext, we will begin by creating the data itself. I chose to make a data set on an imaginary disease in an imaginary area. Each district has their region type (urban, suburban, or rural), case count, vaccination rate, population density, attack rate, and level of intervention (none, partial, or full) recorded. While in real life, these variables can heavily affect one , and especially the case count, for this exercise I chose to focus on a few interactions.\nFirst, population density is heavily dependent on the region type. Next, attack rates are affected by population density and intervention level. Lastly, the case count is affected by population density, vaccination rates, and level of intervention. At the end of this block, I created bounds that try to align with what is logically expected for these values.\n\n# create empty data frame with placeholders for variables\n\nsyn_data &lt;- data.frame(\n  DistrictID = numeric(n_observations),\n  RegionType = character(n_observations),\n  CaseCount = numeric(n_observations),\n  VaccinationRate = numeric(n_observations),\n  PopulationDensity = numeric(n_observations),\n  AttackRate = numeric(n_observations),\n  Intervention = character(n_observations)\n)\n\n# Variable 1: District ID\nsyn_data$DistrictID &lt;- 1:n_observations\n\n# Variable 2: Region Type (Categorical variable)\nsyn_data$RegionType &lt;- purrr::map_chr(sample(c(\"Urban\", \"Rural\", \"Suburban\"), n_observations, replace = TRUE), as.character)\n\n\n\n# Variable 4: Vaccination Rate\nsyn_data$VaccinationRate &lt;- runif(n_observations, min = 0.5, max = 1.0)\n\n# Variable 5: Population Density (per km squared)\nsyn_data$PopulationDensity &lt;- with(syn_data, ifelse(\n  RegionType == \"Urban\", rnorm(sum(RegionType == \"Urban\"), mean = 3000, sd = 500),\n  ifelse(RegionType == \"Suburban\", rnorm(sum(RegionType == \"Suburban\"), mean = 1000, sd = 300),\n         rnorm(sum(RegionType == \"Rural\"), mean = 100, sd = 50)\n  )\n))\n\n# Variable 7: Level of Public Health Intervention\nsyn_data$Intervention &lt;- purrr::map_chr(sample(c(\"None\", \"Partial\", \"Full\"), n_observations, replace = TRUE), as.character)\n\n# Variable 6: Attack Rate (Assisted with by ChatGPT)\nsyn_data$AttackRate &lt;- ifelse(\n  syn_data$RegionType == \"Urban\",\n  runif(n_observations, min = 0.05, max = 0.2) * ifelse(syn_data$Intervention == \"Full\", 0.7, 1.2),\n  runif(n_observations, min = 0.01, max = 0.15)\n)\n\n\n# Variable 3: Case Count (Numerical Variable) Assisted with by ChatGPT (moved down here to follow coding flow)\nsyn_data$CaseCount &lt;- round(\n  (200 / (syn_data$VaccinationRate * 2)) * \n  (syn_data$PopulationDensity / 1000) * \n  ifelse(syn_data$Intervention == \"Full\", 0.5, \n         ifelse(syn_data$Intervention == \"Partial\", 0.8, 1.0))\n)\n\n# Ensure logical bounds\nsyn_data$CaseCount &lt;- pmax(syn_data$CaseCount, 0)\nsyn_data$AttackRate &lt;- round(pmax(pmin(syn_data$AttackRate, 1), 0.01), 2)\nsyn_data$PopulationDensity &lt;- pmax(syn_data$PopulationDensity, 50)  # Density cannot be negative\nsyn_data$VaccinationRate &lt;- round(syn_data$VaccinationRate, 2)  # Round vaccination rates\nsyn_data$PopulationDensity &lt;- pmax(syn_data$PopulationDensity, 10)\n\n\n\n\nIn this section, I begin by looking at summary statistic for the data. After that, I begin exploring the data visually through various box and scatter plots.\n\n# Summary of data\nsummary(syn_data)\n\n   DistrictID      RegionType          CaseCount     VaccinationRate \n Min.   :  1.00   Length:100         Min.   :  3.0   Min.   :0.5100  \n 1st Qu.: 25.75   Class :character   1st Qu.: 15.0   1st Qu.:0.6375  \n Median : 50.50   Mode  :character   Median :100.0   Median :0.7450  \n Mean   : 50.50                      Mean   :145.1   Mean   :0.7531  \n 3rd Qu.: 75.25                      3rd Qu.:226.2   3rd Qu.:0.8500  \n Max.   :100.00                      Max.   :589.0   Max.   :0.9900  \n PopulationDensity   AttackRate     Intervention      \n Min.   :  50.0    Min.   :0.0100   Length:100        \n 1st Qu.: 123.0    1st Qu.:0.0600   Class :character  \n Median : 987.8    Median :0.0900   Mode  :character  \n Mean   :1380.6    Mean   :0.0973                     \n 3rd Qu.:2599.9    3rd Qu.:0.1300                     \n Max.   :3954.6    Max.   :0.2300                     \n\ndplyr::glimpse(syn_data) \n\nRows: 100\nColumns: 7\n$ DistrictID        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ RegionType        &lt;chr&gt; \"Suburban\", \"Suburban\", \"Suburban\", \"Rural\", \"Suburb…\n$ CaseCount         &lt;dbl&gt; 123, 28, 167, 10, 108, 5, 15, 6, 67, 291, 12, 5, 344…\n$ VaccinationRate   &lt;dbl&gt; 0.68, 0.99, 0.58, 0.55, 0.57, 0.85, 0.81, 0.95, 0.84…\n$ PopulationDensity &lt;dbl&gt; 840.72804, 561.47332, 1206.37503, 56.72436, 613.8908…\n$ AttackRate        &lt;dbl&gt; 0.14, 0.03, 0.10, 0.03, 0.01, 0.08, 0.02, 0.05, 0.07…\n$ Intervention      &lt;chr&gt; \"None\", \"Full\", \"Partial\", \"None\", \"None\", \"Full\", \"…\n\n# Region Case Count\nregion_cases &lt;- ggplot(syn_data, aes(x = RegionType, y = CaseCount, fill = RegionType)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Region Type\",\n                        x = \"Region Type\",\n                        y = \"Case Count\")\n\nprint(region_cases)\n\n\n\n\n\n\n\n# Intervention x Case Count\nintervention_cases &lt;- ggplot(syn_data, aes(x = Intervention, y = CaseCount, fill = Intervention)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Intervention Level\",\n                        x = \"Intervention Level\",\n                        y = \"Case Count\")\n\nprint(intervention_cases)\n\n\n\n\n\n\n\n# Region x Intervention x Case Count\nregion_intervention_cases &lt;- ggplot(syn_data, aes(x = RegionType, y = CaseCount, fill = Intervention)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Region Type and Intervention level\",\n                        x = \"Region Type\",\n                        y = \"Case Count\")\n\nprint(region_intervention_cases)\n\n\n\n\n\n\n\n# Population Density x Case Count\ndensity_cases &lt;- ggplot(syn_data, aes(x = PopulationDensity, y =CaseCount)) + geom_point() + theme_minimal() +\n    labs(title = \"Case Count by Population Density\",\n         x = \"Population Density (per km squared and Log Scaled)\",\n         y = \"Case Count\") +\n    scale_x_log10()\n\nprint(density_cases)\n\n\n\n\n\n\n\n\n\n\n\nFor the final part of this mini-exploration, I chose to conduct three analyses.\nFirst, I did a linear model with the 3 variables, population density, vaccination rate, and intervention. The adjusted R-squared of 0.9313 indicates that, after accounting for the number of predictors, 93.13% of the variance in case count is explained by the variables.\nNext, I looked at whether intervention levels significantly affected the case count of a district by using ANOVA. The p-value of 0.0104 indicates that there is a significant differences among intervention levels.\nLastly, principle component analysis was conducted for exploration purposes. The results indicate that PC1, Population Density, explains 65.1% of the variation seen in the case counts. PC2, vaccination rate, explains 33.12% and PC3, Intervention Level, explains 1.774%. With this in mind, models in the future can consider omitting PC3 because of its small contribution to variance.\n\n# Creation of Linear Model with 3 variables, population density, vaccination rate, and intervention\nlm_model_everything &lt;- lm(CaseCount ~ PopulationDensity + VaccinationRate + Intervention, data = syn_data)\nsummary(lm_model_everything)\n\n\nCall:\nlm(formula = CaseCount ~ PopulationDensity + VaccinationRate + \n    Intervention, data = syn_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.268 -20.001  -5.863  23.237 156.904 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.053e+01  2.377e+01   2.126 0.036077 *  \nPopulationDensity    1.057e-01  3.061e-03  34.531  &lt; 2e-16 ***\nVaccinationRate     -1.195e+02  2.974e+01  -4.018 0.000118 ***\nInterventionNone     7.019e+01  1.003e+01   6.999 3.63e-10 ***\nInterventionPartial  4.907e+01  9.107e+00   5.388 5.16e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.43 on 95 degrees of freedom\nMultiple R-squared:  0.934, Adjusted R-squared:  0.9313 \nF-statistic: 336.3 on 4 and 95 DF,  p-value: &lt; 2.2e-16\n\n# Creation of ANOVA to test if intervention levels signficantly affect case count\nanova_model &lt;- aov(CaseCount ~ Intervention, data = syn_data)\nsummary(anova_model)\n\n             Df  Sum Sq Mean Sq F value Pr(&gt;F)  \nIntervention  2  191028   95514   4.785 0.0104 *\nResiduals    97 1936247   19961                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Principle Component Analysis (suggested by ChatGPT) to view which variables explain the variance\npca_model &lt;- prcomp(syn_data[, c(\"PopulationDensity\", \"VaccinationRate\", \"CaseCount\")], center = TRUE, scale. = TRUE)\nsummary(pca_model)\n\nImportance of components:\n                         PC1    PC2     PC3\nStandard deviation     1.398 0.9968 0.23067\nProportion of Variance 0.651 0.3312 0.01774\nCumulative Proportion  0.651 0.9823 1.00000"
  },
  {
    "objectID": "data-exercise/data-exercise.qmd.html#package-loading",
    "href": "data-exercise/data-exercise.qmd.html#package-loading",
    "title": "Data Exercise",
    "section": "",
    "text": "First, we will start with loading some necessary packages for data creation, visualization, and more.\n\n# loading dslabs package\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(here)\n\nhere() starts at C:/Users/vince/OneDrive/Desktop/port-MADA/vincentnguyen-MADA-portfolio"
  },
  {
    "objectID": "data-exercise/data-exercise.qmd.html#seed-setting",
    "href": "data-exercise/data-exercise.qmd.html#seed-setting",
    "title": "Data Exercise",
    "section": "",
    "text": "Since the process is randomized, setting a seed can help improve reproduciblity when creating synthetic data.\n\n# set a seed for reproducibility\nset.seed(123)\n\n# define number of observations\nn_observations &lt;- 100"
  },
  {
    "objectID": "data-exercise/data-exercise.qmd.html#data-creation",
    "href": "data-exercise/data-exercise.qmd.html#data-creation",
    "title": "Data Exercise",
    "section": "",
    "text": "Next, we will begin by creating the data itself. I chose to make a data set on an imaginary disease in an imaginary area. Each district has their region type (urban, suburban, or rural), case count, vaccination rate, population density, attack rate, and level of intervention (none, partial, or full) recorded. While in real life, these variables can heavily affect one , and especially the case count, for this exercise I chose to focus on a few interactions.\nFirst, population density is heavily dependent on the region type. Next, attack rates are affected by population density and intervention level. Lastly, the case count is affected by population density, vaccination rates, and level of intervention. At the end of this block, I created bounds that try to align with what is logically expected for these values.\n\n# create empty data frame with placeholders for variables\n\nsyn_data &lt;- data.frame(\n  DistrictID = numeric(n_observations),\n  RegionType = character(n_observations),\n  CaseCount = numeric(n_observations),\n  VaccinationRate = numeric(n_observations),\n  PopulationDensity = numeric(n_observations),\n  AttackRate = numeric(n_observations),\n  Intervention = character(n_observations)\n)\n\n# Variable 1: District ID\nsyn_data$DistrictID &lt;- 1:n_observations\n\n# Variable 2: Region Type (Categorical variable)\nsyn_data$RegionType &lt;- purrr::map_chr(sample(c(\"Urban\", \"Rural\", \"Suburban\"), n_observations, replace = TRUE), as.character)\n\n\n\n# Variable 4: Vaccination Rate\nsyn_data$VaccinationRate &lt;- runif(n_observations, min = 0.5, max = 1.0)\n\n# Variable 5: Population Density (per km squared)\nsyn_data$PopulationDensity &lt;- with(syn_data, ifelse(\n  RegionType == \"Urban\", rnorm(sum(RegionType == \"Urban\"), mean = 3000, sd = 500),\n  ifelse(RegionType == \"Suburban\", rnorm(sum(RegionType == \"Suburban\"), mean = 1000, sd = 300),\n         rnorm(sum(RegionType == \"Rural\"), mean = 100, sd = 50)\n  )\n))\n\n# Variable 7: Level of Public Health Intervention\nsyn_data$Intervention &lt;- purrr::map_chr(sample(c(\"None\", \"Partial\", \"Full\"), n_observations, replace = TRUE), as.character)\n\n# Variable 6: Attack Rate (Assisted with by ChatGPT)\nsyn_data$AttackRate &lt;- ifelse(\n  syn_data$RegionType == \"Urban\",\n  runif(n_observations, min = 0.05, max = 0.2) * ifelse(syn_data$Intervention == \"Full\", 0.7, 1.2),\n  runif(n_observations, min = 0.01, max = 0.15)\n)\n\n\n# Variable 3: Case Count (Numerical Variable) Assisted with by ChatGPT (moved down here to follow coding flow)\nsyn_data$CaseCount &lt;- round(\n  (200 / (syn_data$VaccinationRate * 2)) * \n  (syn_data$PopulationDensity / 1000) * \n  ifelse(syn_data$Intervention == \"Full\", 0.5, \n         ifelse(syn_data$Intervention == \"Partial\", 0.8, 1.0))\n)\n\n# Ensure logical bounds\nsyn_data$CaseCount &lt;- pmax(syn_data$CaseCount, 0)\nsyn_data$AttackRate &lt;- round(pmax(pmin(syn_data$AttackRate, 1), 0.01), 2)\nsyn_data$PopulationDensity &lt;- pmax(syn_data$PopulationDensity, 50)  # Density cannot be negative\nsyn_data$VaccinationRate &lt;- round(syn_data$VaccinationRate, 2)  # Round vaccination rates\nsyn_data$PopulationDensity &lt;- pmax(syn_data$PopulationDensity, 10)"
  },
  {
    "objectID": "data-exercise/data-exercise.qmd.html#data-exploration",
    "href": "data-exercise/data-exercise.qmd.html#data-exploration",
    "title": "Data Exercise",
    "section": "",
    "text": "In this section, I begin by looking at summary statistic for the data. After that, I begin exploring the data visually through various box and scatter plots.\n\n# Summary of data\nsummary(syn_data)\n\n   DistrictID      RegionType          CaseCount     VaccinationRate \n Min.   :  1.00   Length:100         Min.   :  3.0   Min.   :0.5100  \n 1st Qu.: 25.75   Class :character   1st Qu.: 15.0   1st Qu.:0.6375  \n Median : 50.50   Mode  :character   Median :100.0   Median :0.7450  \n Mean   : 50.50                      Mean   :145.1   Mean   :0.7531  \n 3rd Qu.: 75.25                      3rd Qu.:226.2   3rd Qu.:0.8500  \n Max.   :100.00                      Max.   :589.0   Max.   :0.9900  \n PopulationDensity   AttackRate     Intervention      \n Min.   :  50.0    Min.   :0.0100   Length:100        \n 1st Qu.: 123.0    1st Qu.:0.0600   Class :character  \n Median : 987.8    Median :0.0900   Mode  :character  \n Mean   :1380.6    Mean   :0.0973                     \n 3rd Qu.:2599.9    3rd Qu.:0.1300                     \n Max.   :3954.6    Max.   :0.2300                     \n\ndplyr::glimpse(syn_data) \n\nRows: 100\nColumns: 7\n$ DistrictID        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ RegionType        &lt;chr&gt; \"Suburban\", \"Suburban\", \"Suburban\", \"Rural\", \"Suburb…\n$ CaseCount         &lt;dbl&gt; 123, 28, 167, 10, 108, 5, 15, 6, 67, 291, 12, 5, 344…\n$ VaccinationRate   &lt;dbl&gt; 0.68, 0.99, 0.58, 0.55, 0.57, 0.85, 0.81, 0.95, 0.84…\n$ PopulationDensity &lt;dbl&gt; 840.72804, 561.47332, 1206.37503, 56.72436, 613.8908…\n$ AttackRate        &lt;dbl&gt; 0.14, 0.03, 0.10, 0.03, 0.01, 0.08, 0.02, 0.05, 0.07…\n$ Intervention      &lt;chr&gt; \"None\", \"Full\", \"Partial\", \"None\", \"None\", \"Full\", \"…\n\n# Region Case Count\nregion_cases &lt;- ggplot(syn_data, aes(x = RegionType, y = CaseCount, fill = RegionType)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Region Type\",\n                        x = \"Region Type\",\n                        y = \"Case Count\")\n\nprint(region_cases)\n\n\n\n\n\n\n\n# Intervention x Case Count\nintervention_cases &lt;- ggplot(syn_data, aes(x = Intervention, y = CaseCount, fill = Intervention)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Intervention Level\",\n                        x = \"Intervention Level\",\n                        y = \"Case Count\")\n\nprint(intervention_cases)\n\n\n\n\n\n\n\n# Region x Intervention x Case Count\nregion_intervention_cases &lt;- ggplot(syn_data, aes(x = RegionType, y = CaseCount, fill = Intervention)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Region Type and Intervention level\",\n                        x = \"Region Type\",\n                        y = \"Case Count\")\n\nprint(region_intervention_cases)\n\n\n\n\n\n\n\n# Population Density x Case Count\ndensity_cases &lt;- ggplot(syn_data, aes(x = PopulationDensity, y =CaseCount)) + geom_point() + theme_minimal() +\n    labs(title = \"Case Count by Population Density\",\n         x = \"Population Density (per km squared and Log Scaled)\",\n         y = \"Case Count\") +\n    scale_x_log10()\n\nprint(density_cases)"
  },
  {
    "objectID": "data-exercise/data-exercise.qmd.html#model-creation",
    "href": "data-exercise/data-exercise.qmd.html#model-creation",
    "title": "Data Exercise",
    "section": "",
    "text": "For the final part of this mini-exploration, I chose to conduct three analyses.\nFirst, I did a linear model with the 3 variables, population density, vaccination rate, and intervention. The adjusted R-squared of 0.9313 indicates that, after accounting for the number of predictors, 93.13% of the variance in case count is explained by the variables.\nNext, I looked at whether intervention levels significantly affected the case count of a district by using ANOVA. The p-value of 0.0104 indicates that there is a significant differences among intervention levels.\nLastly, principle component analysis was conducted for exploration purposes. The results indicate that PC1, Population Density, explains 65.1% of the variation seen in the case counts. PC2, vaccination rate, explains 33.12% and PC3, Intervention Level, explains 1.774%. With this in mind, models in the future can consider omitting PC3 because of its small contribution to variance.\n\n# Creation of Linear Model with 3 variables, population density, vaccination rate, and intervention\nlm_model_everything &lt;- lm(CaseCount ~ PopulationDensity + VaccinationRate + Intervention, data = syn_data)\nsummary(lm_model_everything)\n\n\nCall:\nlm(formula = CaseCount ~ PopulationDensity + VaccinationRate + \n    Intervention, data = syn_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.268 -20.001  -5.863  23.237 156.904 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.053e+01  2.377e+01   2.126 0.036077 *  \nPopulationDensity    1.057e-01  3.061e-03  34.531  &lt; 2e-16 ***\nVaccinationRate     -1.195e+02  2.974e+01  -4.018 0.000118 ***\nInterventionNone     7.019e+01  1.003e+01   6.999 3.63e-10 ***\nInterventionPartial  4.907e+01  9.107e+00   5.388 5.16e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.43 on 95 degrees of freedom\nMultiple R-squared:  0.934, Adjusted R-squared:  0.9313 \nF-statistic: 336.3 on 4 and 95 DF,  p-value: &lt; 2.2e-16\n\n# Creation of ANOVA to test if intervention levels signficantly affect case count\nanova_model &lt;- aov(CaseCount ~ Intervention, data = syn_data)\nsummary(anova_model)\n\n             Df  Sum Sq Mean Sq F value Pr(&gt;F)  \nIntervention  2  191028   95514   4.785 0.0104 *\nResiduals    97 1936247   19961                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Principle Component Analysis (suggested by ChatGPT) to view which variables explain the variance\npca_model &lt;- prcomp(syn_data[, c(\"PopulationDensity\", \"VaccinationRate\", \"CaseCount\")], center = TRUE, scale. = TRUE)\nsummary(pca_model)\n\nImportance of components:\n                         PC1    PC2     PC3\nStandard deviation     1.398 0.9968 0.23067\nProportion of Variance 0.651 0.3312 0.01774\nCumulative Proportion  0.651 0.9823 1.00000"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "First, we will start with loading some necessary packages for data creation, visualization, and more.\n\n# loading dslabs package\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/vince/OneDrive/Desktop/port-MADA/vincentnguyen-MADA-portfolio\n\n\n\n\n\nSince the process is randomized, setting a seed can help improve reproduciblity when creating synthetic data.\n\n# set a seed for reproducibility\nset.seed(123)\n\n# define number of observations\nn_observations &lt;- 100\n\n\n\n\nNext, we will begin by creating the data itself. I chose to make a data set on an imaginary disease in an imaginary area. Each district has their region type (urban, suburban, or rural), case count, vaccination rate, population density, attack rate, and level of intervention (none, partial, or full) recorded. While in real life, these variables can heavily affect one , and especially the case count, for this exercise I chose to focus on a few interactions.\nFirst, population density is heavily dependent on the region type. Next, attack rates are affected by population density and intervention level. Lastly, the case count is affected by population density, vaccination rates, and level of intervention. At the end of this block, I created bounds that try to align with what is logically expected for these values.\n\n# create empty data frame with placeholders for variables\n\nsyn_data &lt;- data.frame(\n  DistrictID = numeric(n_observations),\n  RegionType = character(n_observations),\n  CaseCount = numeric(n_observations),\n  VaccinationRate = numeric(n_observations),\n  PopulationDensity = numeric(n_observations),\n  AttackRate = numeric(n_observations),\n  Intervention = character(n_observations)\n)\n\n# Variable 1: District ID\nsyn_data$DistrictID &lt;- 1:n_observations\n\n# Variable 2: Region Type (Categorical variable)\nsyn_data$RegionType &lt;- purrr::map_chr(sample(c(\"Urban\", \"Rural\", \"Suburban\"), n_observations, replace = TRUE), as.character)\n\n\n\n# Variable 4: Vaccination Rate\nsyn_data$VaccinationRate &lt;- runif(n_observations, min = 0.5, max = 1.0)\n\n# Variable 5: Population Density (per km squared)\nsyn_data$PopulationDensity &lt;- with(syn_data, ifelse(\n  RegionType == \"Urban\", rnorm(sum(RegionType == \"Urban\"), mean = 3000, sd = 500),\n  ifelse(RegionType == \"Suburban\", rnorm(sum(RegionType == \"Suburban\"), mean = 1000, sd = 300),\n         rnorm(sum(RegionType == \"Rural\"), mean = 100, sd = 50)\n  )\n))\n\n# Variable 7: Level of Public Health Intervention\nsyn_data$Intervention &lt;- purrr::map_chr(sample(c(\"None\", \"Partial\", \"Full\"), n_observations, replace = TRUE), as.character)\n\n# Variable 6: Attack Rate (Assisted with by ChatGPT)\nsyn_data$AttackRate &lt;- ifelse(\n  syn_data$RegionType == \"Urban\",\n  runif(n_observations, min = 0.05, max = 0.2) * ifelse(syn_data$Intervention == \"Full\", 0.7, 1.2),\n  runif(n_observations, min = 0.01, max = 0.15)\n)\n\n\n# Variable 3: Case Count (Numerical Variable) Assisted with by ChatGPT (moved down here to follow coding flow)\nsyn_data$CaseCount &lt;- round(\n  (200 / (syn_data$VaccinationRate * 2)) * \n  (syn_data$PopulationDensity / 1000) * \n  ifelse(syn_data$Intervention == \"Full\", 0.5, \n         ifelse(syn_data$Intervention == \"Partial\", 0.8, 1.0))\n)\n\n# Ensure logical bounds\nsyn_data$CaseCount &lt;- pmax(syn_data$CaseCount, 0)\nsyn_data$AttackRate &lt;- round(pmax(pmin(syn_data$AttackRate, 1), 0.01), 2)\nsyn_data$PopulationDensity &lt;- pmax(syn_data$PopulationDensity, 50)  # Density cannot be negative\nsyn_data$VaccinationRate &lt;- round(syn_data$VaccinationRate, 2)  # Round vaccination rates\nsyn_data$PopulationDensity &lt;- pmax(syn_data$PopulationDensity, 10)\n\n\n\n\nIn this section, I begin by looking at summary statistic for the data. After that, I begin exploring the data visually through various box and scatter plots.\n\n# Summary of data\nsummary(syn_data)\n\n   DistrictID      RegionType          CaseCount     VaccinationRate \n Min.   :  1.00   Length:100         Min.   :  3.0   Min.   :0.5100  \n 1st Qu.: 25.75   Class :character   1st Qu.: 15.0   1st Qu.:0.6375  \n Median : 50.50   Mode  :character   Median :100.0   Median :0.7450  \n Mean   : 50.50                      Mean   :145.1   Mean   :0.7531  \n 3rd Qu.: 75.25                      3rd Qu.:226.2   3rd Qu.:0.8500  \n Max.   :100.00                      Max.   :589.0   Max.   :0.9900  \n PopulationDensity   AttackRate     Intervention      \n Min.   :  50.0    Min.   :0.0100   Length:100        \n 1st Qu.: 123.0    1st Qu.:0.0600   Class :character  \n Median : 987.8    Median :0.0900   Mode  :character  \n Mean   :1380.6    Mean   :0.0973                     \n 3rd Qu.:2599.9    3rd Qu.:0.1300                     \n Max.   :3954.6    Max.   :0.2300                     \n\ndplyr::glimpse(syn_data) \n\nRows: 100\nColumns: 7\n$ DistrictID        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ RegionType        &lt;chr&gt; \"Suburban\", \"Suburban\", \"Suburban\", \"Rural\", \"Suburb…\n$ CaseCount         &lt;dbl&gt; 123, 28, 167, 10, 108, 5, 15, 6, 67, 291, 12, 5, 344…\n$ VaccinationRate   &lt;dbl&gt; 0.68, 0.99, 0.58, 0.55, 0.57, 0.85, 0.81, 0.95, 0.84…\n$ PopulationDensity &lt;dbl&gt; 840.72804, 561.47332, 1206.37503, 56.72436, 613.8908…\n$ AttackRate        &lt;dbl&gt; 0.14, 0.03, 0.10, 0.03, 0.01, 0.08, 0.02, 0.05, 0.07…\n$ Intervention      &lt;chr&gt; \"None\", \"Full\", \"Partial\", \"None\", \"None\", \"Full\", \"…\n\n# Region Case Count\nregion_cases &lt;- ggplot(syn_data, aes(x = RegionType, y = CaseCount, fill = RegionType)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Region Type\",\n                        x = \"Region Type\",\n                        y = \"Case Count\")\n\nprint(region_cases)\n\n\n\n\n\n\n\n# Intervention x Case Count\nintervention_cases &lt;- ggplot(syn_data, aes(x = Intervention, y = CaseCount, fill = Intervention)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Intervention Level\",\n                        x = \"Intervention Level\",\n                        y = \"Case Count\")\n\nprint(intervention_cases)\n\n\n\n\n\n\n\n# Region x Intervention x Case Count\nregion_intervention_cases &lt;- ggplot(syn_data, aes(x = RegionType, y = CaseCount, fill = Intervention)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Region Type and Intervention level\",\n                        x = \"Region Type\",\n                        y = \"Case Count\")\n\nprint(region_intervention_cases)\n\n\n\n\n\n\n\n# Population Density x Case Count\ndensity_cases &lt;- ggplot(syn_data, aes(x = PopulationDensity, y =CaseCount)) + geom_point() + theme_minimal() +\n    labs(title = \"Case Count by Population Density\",\n         x = \"Population Density (per km squared and Log Scaled)\",\n         y = \"Case Count\") +\n    scale_x_log10()\n\nprint(density_cases)\n\n\n\n\n\n\n\n\n\n\n\nFor the final part of this mini-exploration, I chose to conduct three analyses.\nFirst, I did a linear model with the 3 variables, population density, vaccination rate, and intervention. The adjusted R-squared of 0.9313 indicates that, after accounting for the number of predictors, 93.13% of the variance in case count is explained by the variables.\nNext, I looked at whether intervention levels significantly affected the case count of a district by using ANOVA. The p-value of 0.0104 indicates that there is a significant differences among intervention levels.\nLastly, principle component analysis was conducted for exploration purposes. The results indicate that PC1, Population Density, explains 65.1% of the variation seen in the case counts. PC2, vaccination rate, explains 33.12% and PC3, Intervention Level, explains 1.774%. With this in mind, models in the future can consider omitting PC3 because of its small contribution to variance.\n\n# Creation of Linear Model with 3 variables, population density, vaccination rate, and intervention\nlm_model_everything &lt;- lm(CaseCount ~ PopulationDensity + VaccinationRate + Intervention, data = syn_data)\nsummary(lm_model_everything)\n\n\nCall:\nlm(formula = CaseCount ~ PopulationDensity + VaccinationRate + \n    Intervention, data = syn_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.268 -20.001  -5.863  23.237 156.904 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.053e+01  2.377e+01   2.126 0.036077 *  \nPopulationDensity    1.057e-01  3.061e-03  34.531  &lt; 2e-16 ***\nVaccinationRate     -1.195e+02  2.974e+01  -4.018 0.000118 ***\nInterventionNone     7.019e+01  1.003e+01   6.999 3.63e-10 ***\nInterventionPartial  4.907e+01  9.107e+00   5.388 5.16e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.43 on 95 degrees of freedom\nMultiple R-squared:  0.934, Adjusted R-squared:  0.9313 \nF-statistic: 336.3 on 4 and 95 DF,  p-value: &lt; 2.2e-16\n\n# Creation of ANOVA to test if intervention levels signficantly affect case count\nanova_model &lt;- aov(CaseCount ~ Intervention, data = syn_data)\nsummary(anova_model)\n\n             Df  Sum Sq Mean Sq F value Pr(&gt;F)  \nIntervention  2  191028   95514   4.785 0.0104 *\nResiduals    97 1936247   19961                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Principle Component Analysis (suggested by ChatGPT) to view which variables explain the variance\npca_model &lt;- prcomp(syn_data[, c(\"PopulationDensity\", \"VaccinationRate\", \"CaseCount\")], center = TRUE, scale. = TRUE)\nsummary(pca_model)\n\nImportance of components:\n                         PC1    PC2     PC3\nStandard deviation     1.398 0.9968 0.23067\nProportion of Variance 0.651 0.3312 0.01774\nCumulative Proportion  0.651 0.9823 1.00000"
  },
  {
    "objectID": "data-exercise/data-exercise.html#package-loading",
    "href": "data-exercise/data-exercise.html#package-loading",
    "title": "Data Exercise",
    "section": "",
    "text": "First, we will start with loading some necessary packages for data creation, visualization, and more.\n\n# loading dslabs package\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/vince/OneDrive/Desktop/port-MADA/vincentnguyen-MADA-portfolio"
  },
  {
    "objectID": "data-exercise/data-exercise.html#seed-setting",
    "href": "data-exercise/data-exercise.html#seed-setting",
    "title": "Data Exercise",
    "section": "",
    "text": "Since the process is randomized, setting a seed can help improve reproduciblity when creating synthetic data.\n\n# set a seed for reproducibility\nset.seed(123)\n\n# define number of observations\nn_observations &lt;- 100"
  },
  {
    "objectID": "data-exercise/data-exercise.html#data-creation",
    "href": "data-exercise/data-exercise.html#data-creation",
    "title": "Data Exercise",
    "section": "",
    "text": "Next, we will begin by creating the data itself. I chose to make a data set on an imaginary disease in an imaginary area. Each district has their region type (urban, suburban, or rural), case count, vaccination rate, population density, attack rate, and level of intervention (none, partial, or full) recorded. While in real life, these variables can heavily affect one , and especially the case count, for this exercise I chose to focus on a few interactions.\nFirst, population density is heavily dependent on the region type. Next, attack rates are affected by population density and intervention level. Lastly, the case count is affected by population density, vaccination rates, and level of intervention. At the end of this block, I created bounds that try to align with what is logically expected for these values.\n\n# create empty data frame with placeholders for variables\n\nsyn_data &lt;- data.frame(\n  DistrictID = numeric(n_observations),\n  RegionType = character(n_observations),\n  CaseCount = numeric(n_observations),\n  VaccinationRate = numeric(n_observations),\n  PopulationDensity = numeric(n_observations),\n  AttackRate = numeric(n_observations),\n  Intervention = character(n_observations)\n)\n\n# Variable 1: District ID\nsyn_data$DistrictID &lt;- 1:n_observations\n\n# Variable 2: Region Type (Categorical variable)\nsyn_data$RegionType &lt;- purrr::map_chr(sample(c(\"Urban\", \"Rural\", \"Suburban\"), n_observations, replace = TRUE), as.character)\n\n\n\n# Variable 4: Vaccination Rate\nsyn_data$VaccinationRate &lt;- runif(n_observations, min = 0.5, max = 1.0)\n\n# Variable 5: Population Density (per km squared)\nsyn_data$PopulationDensity &lt;- with(syn_data, ifelse(\n  RegionType == \"Urban\", rnorm(sum(RegionType == \"Urban\"), mean = 3000, sd = 500),\n  ifelse(RegionType == \"Suburban\", rnorm(sum(RegionType == \"Suburban\"), mean = 1000, sd = 300),\n         rnorm(sum(RegionType == \"Rural\"), mean = 100, sd = 50)\n  )\n))\n\n# Variable 7: Level of Public Health Intervention\nsyn_data$Intervention &lt;- purrr::map_chr(sample(c(\"None\", \"Partial\", \"Full\"), n_observations, replace = TRUE), as.character)\n\n# Variable 6: Attack Rate (Assisted with by ChatGPT)\nsyn_data$AttackRate &lt;- ifelse(\n  syn_data$RegionType == \"Urban\",\n  runif(n_observations, min = 0.05, max = 0.2) * ifelse(syn_data$Intervention == \"Full\", 0.7, 1.2),\n  runif(n_observations, min = 0.01, max = 0.15)\n)\n\n\n# Variable 3: Case Count (Numerical Variable) Assisted with by ChatGPT (moved down here to follow coding flow)\nsyn_data$CaseCount &lt;- round(\n  (200 / (syn_data$VaccinationRate * 2)) * \n  (syn_data$PopulationDensity / 1000) * \n  ifelse(syn_data$Intervention == \"Full\", 0.5, \n         ifelse(syn_data$Intervention == \"Partial\", 0.8, 1.0))\n)\n\n# Ensure logical bounds\nsyn_data$CaseCount &lt;- pmax(syn_data$CaseCount, 0)\nsyn_data$AttackRate &lt;- round(pmax(pmin(syn_data$AttackRate, 1), 0.01), 2)\nsyn_data$PopulationDensity &lt;- pmax(syn_data$PopulationDensity, 50)  # Density cannot be negative\nsyn_data$VaccinationRate &lt;- round(syn_data$VaccinationRate, 2)  # Round vaccination rates\nsyn_data$PopulationDensity &lt;- pmax(syn_data$PopulationDensity, 10)"
  },
  {
    "objectID": "data-exercise/data-exercise.html#data-exploration",
    "href": "data-exercise/data-exercise.html#data-exploration",
    "title": "Data Exercise",
    "section": "",
    "text": "In this section, I begin by looking at summary statistic for the data. After that, I begin exploring the data visually through various box and scatter plots.\n\n# Summary of data\nsummary(syn_data)\n\n   DistrictID      RegionType          CaseCount     VaccinationRate \n Min.   :  1.00   Length:100         Min.   :  3.0   Min.   :0.5100  \n 1st Qu.: 25.75   Class :character   1st Qu.: 15.0   1st Qu.:0.6375  \n Median : 50.50   Mode  :character   Median :100.0   Median :0.7450  \n Mean   : 50.50                      Mean   :145.1   Mean   :0.7531  \n 3rd Qu.: 75.25                      3rd Qu.:226.2   3rd Qu.:0.8500  \n Max.   :100.00                      Max.   :589.0   Max.   :0.9900  \n PopulationDensity   AttackRate     Intervention      \n Min.   :  50.0    Min.   :0.0100   Length:100        \n 1st Qu.: 123.0    1st Qu.:0.0600   Class :character  \n Median : 987.8    Median :0.0900   Mode  :character  \n Mean   :1380.6    Mean   :0.0973                     \n 3rd Qu.:2599.9    3rd Qu.:0.1300                     \n Max.   :3954.6    Max.   :0.2300                     \n\ndplyr::glimpse(syn_data) \n\nRows: 100\nColumns: 7\n$ DistrictID        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ RegionType        &lt;chr&gt; \"Suburban\", \"Suburban\", \"Suburban\", \"Rural\", \"Suburb…\n$ CaseCount         &lt;dbl&gt; 123, 28, 167, 10, 108, 5, 15, 6, 67, 291, 12, 5, 344…\n$ VaccinationRate   &lt;dbl&gt; 0.68, 0.99, 0.58, 0.55, 0.57, 0.85, 0.81, 0.95, 0.84…\n$ PopulationDensity &lt;dbl&gt; 840.72804, 561.47332, 1206.37503, 56.72436, 613.8908…\n$ AttackRate        &lt;dbl&gt; 0.14, 0.03, 0.10, 0.03, 0.01, 0.08, 0.02, 0.05, 0.07…\n$ Intervention      &lt;chr&gt; \"None\", \"Full\", \"Partial\", \"None\", \"None\", \"Full\", \"…\n\n# Region Case Count\nregion_cases &lt;- ggplot(syn_data, aes(x = RegionType, y = CaseCount, fill = RegionType)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Region Type\",\n                        x = \"Region Type\",\n                        y = \"Case Count\")\n\nprint(region_cases)\n\n\n\n\n\n\n\n# Intervention x Case Count\nintervention_cases &lt;- ggplot(syn_data, aes(x = Intervention, y = CaseCount, fill = Intervention)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Intervention Level\",\n                        x = \"Intervention Level\",\n                        y = \"Case Count\")\n\nprint(intervention_cases)\n\n\n\n\n\n\n\n# Region x Intervention x Case Count\nregion_intervention_cases &lt;- ggplot(syn_data, aes(x = RegionType, y = CaseCount, fill = Intervention)) + geom_boxplot() + theme_minimal() + labs(title = \"Case Count by Region Type and Intervention level\",\n                        x = \"Region Type\",\n                        y = \"Case Count\")\n\nprint(region_intervention_cases)\n\n\n\n\n\n\n\n# Population Density x Case Count\ndensity_cases &lt;- ggplot(syn_data, aes(x = PopulationDensity, y =CaseCount)) + geom_point() + theme_minimal() +\n    labs(title = \"Case Count by Population Density\",\n         x = \"Population Density (per km squared and Log Scaled)\",\n         y = \"Case Count\") +\n    scale_x_log10()\n\nprint(density_cases)"
  },
  {
    "objectID": "data-exercise/data-exercise.html#model-creation",
    "href": "data-exercise/data-exercise.html#model-creation",
    "title": "Data Exercise",
    "section": "",
    "text": "For the final part of this mini-exploration, I chose to conduct three analyses.\nFirst, I did a linear model with the 3 variables, population density, vaccination rate, and intervention. The adjusted R-squared of 0.9313 indicates that, after accounting for the number of predictors, 93.13% of the variance in case count is explained by the variables.\nNext, I looked at whether intervention levels significantly affected the case count of a district by using ANOVA. The p-value of 0.0104 indicates that there is a significant differences among intervention levels.\nLastly, principle component analysis was conducted for exploration purposes. The results indicate that PC1, Population Density, explains 65.1% of the variation seen in the case counts. PC2, vaccination rate, explains 33.12% and PC3, Intervention Level, explains 1.774%. With this in mind, models in the future can consider omitting PC3 because of its small contribution to variance.\n\n# Creation of Linear Model with 3 variables, population density, vaccination rate, and intervention\nlm_model_everything &lt;- lm(CaseCount ~ PopulationDensity + VaccinationRate + Intervention, data = syn_data)\nsummary(lm_model_everything)\n\n\nCall:\nlm(formula = CaseCount ~ PopulationDensity + VaccinationRate + \n    Intervention, data = syn_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.268 -20.001  -5.863  23.237 156.904 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.053e+01  2.377e+01   2.126 0.036077 *  \nPopulationDensity    1.057e-01  3.061e-03  34.531  &lt; 2e-16 ***\nVaccinationRate     -1.195e+02  2.974e+01  -4.018 0.000118 ***\nInterventionNone     7.019e+01  1.003e+01   6.999 3.63e-10 ***\nInterventionPartial  4.907e+01  9.107e+00   5.388 5.16e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.43 on 95 degrees of freedom\nMultiple R-squared:  0.934, Adjusted R-squared:  0.9313 \nF-statistic: 336.3 on 4 and 95 DF,  p-value: &lt; 2.2e-16\n\n# Creation of ANOVA to test if intervention levels signficantly affect case count\nanova_model &lt;- aov(CaseCount ~ Intervention, data = syn_data)\nsummary(anova_model)\n\n             Df  Sum Sq Mean Sq F value Pr(&gt;F)  \nIntervention  2  191028   95514   4.785 0.0104 *\nResiduals    97 1936247   19961                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Principle Component Analysis (suggested by ChatGPT) to view which variables explain the variance\npca_model &lt;- prcomp(syn_data[, c(\"PopulationDensity\", \"VaccinationRate\", \"CaseCount\")], center = TRUE, scale. = TRUE)\nsummary(pca_model)\n\nImportance of components:\n                         PC1    PC2     PC3\nStandard deviation     1.398 0.9968 0.23067\nProportion of Variance 0.651 0.3312 0.01774\nCumulative Proportion  0.651 0.9823 1.00000"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "First, we will start with loading some necessary packages for data creation, visualization, and more.\n\n# loading dslabs package\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/vince/OneDrive/Desktop/port-MADA/vincentnguyen-MADA-portfolio\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.3.3\n\nlibrary(janitor)\n\nWarning: package 'janitor' was built under R version 4.3.3\n\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\n\n\n\nThe data set is titled, “Weekly Provisional Counts of Deaths by State and Select Causes, 2020-2023”. The data set contains 10476 observations of 35 variables. It covers counts of death by nationally or by state. Additionally, it includes causes of death and more.\n\n# Import dataset\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"cdcdata-exercise\", \"Weekly_Provisional_Counts_of_Deaths_by_State_and_Select_Causes__2020-2023_20250204.csv\")\ndata &lt;- read.csv(data_location)%&gt;%\n  clean_names()\n\n# filterd for only the US (no states) and then also removed some diseases not of interest\n# also rename the columns because they are formatted weird\ndata_filtered &lt;- data %&gt;%\n  filter(jurisdiction_of_occurrence == \"United States\") %&gt;%\n  select(1:7, 9:10, 12, 17) %&gt;%\n  rename(\n    cancer_count = malignant_neoplasms_c00_c97,\n    diabetes_count = diabetes_mellitus_e10_e14,\n    influenza_pneumonia_count = influenza_and_pneumonia_j09_j18,\n    heart_disease_count = diseases_of_heart_i00_i09_i11_i13_i20_i51\n  )\n\n\n\n\nCreation of summary statistics for the entire data set and for diseases of interest.\n\n# summary statistics of all causes of death\nsummary(data_filtered)\n\n  data_as_of        jurisdiction_of_occurrence   mmwr_year      mmwr_week    \n Length:194         Length:194                 Min.   :2020   Min.   : 1.00  \n Class :character   Class :character           1st Qu.:2020   1st Qu.:13.00  \n Mode  :character   Mode  :character           Median :2021   Median :25.00  \n                                               Mean   :2021   Mean   :25.21  \n                                               3rd Qu.:2022   3rd Qu.:37.00  \n                                               Max.   :2023   Max.   :53.00  \n week_ending_date     all_cause     natural_cause    cancer_count  \n Length:194         Min.   :37874   Min.   :37824   Min.   : 8675  \n Class :character   1st Qu.:58434   1st Qu.:52449   1st Qu.:11484  \n Mode  :character   Median :60628   Median :54836   Median :11618  \n                    Mean   :63459   Mean   :57802   Mean   :11599  \n                    3rd Qu.:67061   3rd Qu.:61131   3rd Qu.:11781  \n                    Max.   :87415   Max.   :81622   Max.   :12284  \n diabetes_count influenza_pneumonia_count heart_disease_count\n Min.   :1115   Min.   : 495.0            Min.   : 8034      \n 1st Qu.:1800   1st Qu.: 715.2            1st Qu.:12621      \n Median :1895   Median : 792.0            Median :13104      \n Mean   :1923   Mean   : 896.4            Mean   :13255      \n 3rd Qu.:2014   3rd Qu.: 944.5            3rd Qu.:13727      \n Max.   :2601   Max.   :1916.0            Max.   :16538      \n\n# mmean / sd of cancer deaths\nmean(data_filtered$cancer_count)\n\n[1] 11598.83\n\nsd(data_filtered$cancer_count)\n\n[1] 328.8402\n\n# mean / sd of diabetes\nmean(data_filtered$diabetes_count)\n\n[1] 1922.701\n\nsd(data_filtered$diabetes_count)\n\n[1] 212.36\n\n# mean / sd of heart disease\nmean(data_filtered$heart_disease_count)\n\n[1] 13254.74\n\nsd(data_filtered$heart_disease_count)\n\n[1] 1078.617\n\n# mean / sd of influenza and pneumonia\nmean(data_filtered$influenza_pneumonia_count)\n\n[1] 896.4278\n\nsd(data_filtered$influenza_pneumonia_count)\n\n[1] 296.1237\n\n\n\n\n\nCreation of graphs to visualize the distribution of death count cases on a MMWR week-to-week basis. Surprisingly, they look almost normal with some skewing on some of the graphs.\n\n# Distribution graph of cancer deaths\ncancer_graph &lt;- ggplot(data_filtered, aes(x = cancer_count)) +\n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\") +\n  labs(\n    x = \"Cancer Death Count\",\n    y = \"Frequency\",\n    title = \"Distribution of Cancer Death Counts\"\n  )\n\nprint(cancer_graph)\n\n\n\n\n\n\n\n# Distribution graph of diabetes deaths\ndiabetes_graph &lt;- ggplot(data_filtered, aes(x = diabetes_count)) +\n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\") +\n  labs(\n    x = \"Diabetes Death Count\",\n    y = \"Frequency\",\n    title = \"Distribution of Diabetes Death Counts\"\n  )\n\nprint(diabetes_graph)\n\n\n\n\n\n\n\n# Distribution graph of heart disease deaths\nheart_disease_graph &lt;- ggplot(data_filtered, aes(x = heart_disease_count)) +\n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\") +\n  labs(\n    x = \"Heart Disease Death Count\",\n    y = \"Frequency\",\n    title = \"Distribution of Heart Disease Death Counts\"\n  )\n\nprint(heart_disease_graph)\n\n\n\n\n\n\n\n# Distribution graph of influenza/pneumonia deaths\ninfluenza_pneumonia_graph &lt;- ggplot(data_filtered, aes(x = influenza_pneumonia_count)) +\n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\") +\n  labs(\n    x = \"Influenza/Pneumonia Death Count\",\n    y = \"Frequency\",\n    title = \"Distribution of Influenza/Pneumonia Death Counts\"\n  )\n\nprint(influenza_pneumonia_graph)"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#package-loading",
    "href": "cdcdata-exercise/cdcdata-exercise.html#package-loading",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "First, we will start with loading some necessary packages for data creation, visualization, and more.\n\n# loading dslabs package\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/vince/OneDrive/Desktop/port-MADA/vincentnguyen-MADA-portfolio\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.3.3\n\nlibrary(janitor)\n\nWarning: package 'janitor' was built under R version 4.3.3\n\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#data-set",
    "href": "cdcdata-exercise/cdcdata-exercise.html#data-set",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The data set is titled, “Weekly Provisional Counts of Deaths by State and Select Causes, 2020-2023”. The data set contains 10476 observations of 35 variables. It covers counts of death by nationally or by state. Additionally, it includes causes of death and more.\n\n# Import dataset\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"cdcdata-exercise\", \"Weekly_Provisional_Counts_of_Deaths_by_State_and_Select_Causes__2020-2023_20250204.csv\")\ndata &lt;- read.csv(data_location)%&gt;%\n  clean_names()\n\n# filterd for only the US (no states) and then also removed some diseases not of interest\n# also rename the columns because they are formatted weird\ndata_filtered &lt;- data %&gt;%\n  filter(jurisdiction_of_occurrence == \"United States\") %&gt;%\n  select(1:7, 9:10, 12, 17) %&gt;%\n  rename(\n    cancer_count = malignant_neoplasms_c00_c97,\n    diabetes_count = diabetes_mellitus_e10_e14,\n    influenza_pneumonia_count = influenza_and_pneumonia_j09_j18,\n    heart_disease_count = diseases_of_heart_i00_i09_i11_i13_i20_i51\n  )"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#summary-stats",
    "href": "cdcdata-exercise/cdcdata-exercise.html#summary-stats",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "Creation of summary statistics for the entire data set and for diseases of interest.\n\n# summary statistics of all causes of death\nsummary(data_filtered)\n\n  data_as_of        jurisdiction_of_occurrence   mmwr_year      mmwr_week    \n Length:194         Length:194                 Min.   :2020   Min.   : 1.00  \n Class :character   Class :character           1st Qu.:2020   1st Qu.:13.00  \n Mode  :character   Mode  :character           Median :2021   Median :25.00  \n                                               Mean   :2021   Mean   :25.21  \n                                               3rd Qu.:2022   3rd Qu.:37.00  \n                                               Max.   :2023   Max.   :53.00  \n week_ending_date     all_cause     natural_cause    cancer_count  \n Length:194         Min.   :37874   Min.   :37824   Min.   : 8675  \n Class :character   1st Qu.:58434   1st Qu.:52449   1st Qu.:11484  \n Mode  :character   Median :60628   Median :54836   Median :11618  \n                    Mean   :63459   Mean   :57802   Mean   :11599  \n                    3rd Qu.:67061   3rd Qu.:61131   3rd Qu.:11781  \n                    Max.   :87415   Max.   :81622   Max.   :12284  \n diabetes_count influenza_pneumonia_count heart_disease_count\n Min.   :1115   Min.   : 495.0            Min.   : 8034      \n 1st Qu.:1800   1st Qu.: 715.2            1st Qu.:12621      \n Median :1895   Median : 792.0            Median :13104      \n Mean   :1923   Mean   : 896.4            Mean   :13255      \n 3rd Qu.:2014   3rd Qu.: 944.5            3rd Qu.:13727      \n Max.   :2601   Max.   :1916.0            Max.   :16538      \n\n# mmean / sd of cancer deaths\nmean(data_filtered$cancer_count)\n\n[1] 11598.83\n\nsd(data_filtered$cancer_count)\n\n[1] 328.8402\n\n# mean / sd of diabetes\nmean(data_filtered$diabetes_count)\n\n[1] 1922.701\n\nsd(data_filtered$diabetes_count)\n\n[1] 212.36\n\n# mean / sd of heart disease\nmean(data_filtered$heart_disease_count)\n\n[1] 13254.74\n\nsd(data_filtered$heart_disease_count)\n\n[1] 1078.617\n\n# mean / sd of influenza and pneumonia\nmean(data_filtered$influenza_pneumonia_count)\n\n[1] 896.4278\n\nsd(data_filtered$influenza_pneumonia_count)\n\n[1] 296.1237"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#graphs-for-distributions",
    "href": "cdcdata-exercise/cdcdata-exercise.html#graphs-for-distributions",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "Creation of graphs to visualize the distribution of death count cases on a MMWR week-to-week basis. Surprisingly, they look almost normal with some skewing on some of the graphs.\n\n# Distribution graph of cancer deaths\ncancer_graph &lt;- ggplot(data_filtered, aes(x = cancer_count)) +\n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\") +\n  labs(\n    x = \"Cancer Death Count\",\n    y = \"Frequency\",\n    title = \"Distribution of Cancer Death Counts\"\n  )\n\nprint(cancer_graph)\n\n\n\n\n\n\n\n# Distribution graph of diabetes deaths\ndiabetes_graph &lt;- ggplot(data_filtered, aes(x = diabetes_count)) +\n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\") +\n  labs(\n    x = \"Diabetes Death Count\",\n    y = \"Frequency\",\n    title = \"Distribution of Diabetes Death Counts\"\n  )\n\nprint(diabetes_graph)\n\n\n\n\n\n\n\n# Distribution graph of heart disease deaths\nheart_disease_graph &lt;- ggplot(data_filtered, aes(x = heart_disease_count)) +\n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\") +\n  labs(\n    x = \"Heart Disease Death Count\",\n    y = \"Frequency\",\n    title = \"Distribution of Heart Disease Death Counts\"\n  )\n\nprint(heart_disease_graph)\n\n\n\n\n\n\n\n# Distribution graph of influenza/pneumonia deaths\ninfluenza_pneumonia_graph &lt;- ggplot(data_filtered, aes(x = influenza_pneumonia_count)) +\n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\") +\n  labs(\n    x = \"Influenza/Pneumonia Death Count\",\n    y = \"Frequency\",\n    title = \"Distribution of Influenza/Pneumonia Death Counts\"\n  )\n\nprint(influenza_pneumonia_graph)"
  }
]